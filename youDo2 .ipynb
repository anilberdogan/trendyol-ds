{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf91fa1",
   "metadata": {},
   "source": [
    "# You Do 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f5559",
   "metadata": {},
   "source": [
    "Importing libraries and reading movielens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73191ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://files.grouplens.org/datasets/movielens/ml-100k/u.data', delimiter=r'\\t',engine='python',\n",
    "names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "R = df.pivot(index='user_id', columns='item_id', values='rating').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8002e094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  4., ..., nan, nan, nan],\n",
       "       [ 4., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [ 5., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan,  5., nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37ef22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need indexes of non-zero elements to train and score on.\n",
    "irow, jcol = np.where(~np.isnan(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5d626",
   "metadata": {},
   "source": [
    "Train, validation and test split. I used validation set to optimize lambda hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb987013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(R,irow,jcol):\n",
    "    idx_all = np.random.choice(np.arange(100_000), 2000, replace=False)\n",
    "    idx = np.random.choice(np.arange(2000), 1000, replace=False)\n",
    "    val_idx = [val_id for val_id in idx_all if val_id not in idx]\n",
    "    test_irow = irow[idx]\n",
    "    test_jcol = jcol[idx]\n",
    "    val_irow = irow[val_idx]\n",
    "    val_jcol = jcol[val_idx]\n",
    "    R_copy = R.copy()\n",
    "    R_copy[test_irow, test_jcol] = np.nan\n",
    "    R_copy[val_irow, val_jcol] = np.nan\n",
    "    R_test_mask = R.copy() # Will be used only dor prediction\n",
    "    R_test_mask[test_irow, test_jcol] = np.nan\n",
    "    return R_copy, test_irow, test_jcol, val_irow, val_jcol, R_test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434a5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss(b_user, b_item, R, irow, jcol):\n",
    "    loss = 0\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        loss += (R[i, j] - b_user[i] - b_item[j]) ** 2 * 0.5\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244e969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss function\n",
    "def gradient(b_user, b_item, R, irow, jcol):\n",
    "    b_user_grad = np.zeros(b_user.shape)\n",
    "    b_item_grad = np.zeros(b_item.shape)\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        b_user_grad[i] += (R[i, j] - b_user[i] - b_item[j])\n",
    "        b_item_grad[j] += (R[i, j] - b_user[i] - b_item[j])\n",
    "\n",
    "    return b_user_grad, b_item_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da0e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent function\n",
    "def gradient_descent(R, b_user, b_item, irow, jcol, lr, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        b_user_grad, b_item_grad = gradient(b_user, b_item, R, irow, jcol)\n",
    "        prev_b_user = b_user.copy()\n",
    "        prev_b_item = b_item.copy()\n",
    "        b_user += lr * b_user_grad \n",
    "        b_item += lr * b_item_grad\n",
    "        if epoch % 10 == 0:\n",
    "            print('loss:', loss(b_user, b_item, R, irow, jcol))\n",
    "        # early stopping\n",
    "        if np.linalg.norm(b_user - prev_b_user) < 1e-2 and np.linalg.norm(b_item - prev_b_item) < 1e-2:\n",
    "            break\n",
    "\n",
    "    return b_user, b_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c573ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation and test sets\n",
    "R_copy, test_irow, test_jcol, val_irow, val_jcol, R_test_mask = split_data(R,irow,jcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4635cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize biases\n",
    "b_user = np.random.randn(943)\n",
    "b_item = np.random.randn(1682)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e08ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 377212.85336741013\n",
      "loss: 76661.6869947853\n",
      "loss: 57366.645335132285\n",
      "loss: 50445.88796956057\n",
      "loss: 47078.01069912948\n",
      "loss: 45177.82804883845\n",
      "loss: 44000.73670352827\n",
      "loss: 43221.76101390269\n",
      "loss: 42679.889742517276\n",
      "loss: 42287.79542083556\n",
      "loss: 41994.768776683224\n",
      "loss: 41769.754624955014\n",
      "loss: 41592.906266461214\n",
      "loss: 41451.0879914769\n",
      "loss: 41335.34530124266\n",
      "loss: 41239.41648494469\n",
      "loss: 41158.823541521975\n",
      "loss: 41090.29918488571\n",
      "loss: 41031.41581630894\n",
      "loss: 40980.3395703257\n",
      "loss: 40935.66384921904\n",
      "loss: 40896.29453353066\n",
      "loss: 40861.369470052894\n",
      "loss: 40830.20111180686\n",
      "loss: 40802.23505538483\n",
      "loss: 40777.01966112312\n",
      "loss: 40754.183509216055\n",
      "loss: 40733.41846957244\n",
      "loss: 40714.46684344235\n",
      "loss: 40697.11149313042\n",
      "loss: 40681.16818888507\n",
      "loss: 40666.47961830014\n",
      "loss: 40652.91065480279\n",
      "loss: 40640.34458879892\n",
      "loss: 40628.68010146801\n",
      "loss: 40617.82881643971\n",
      "loss: 40607.71330482417\n",
      "loss: 40598.265448644815\n",
      "loss: 40589.425089748926\n",
      "loss: 40581.138907667155\n",
      "loss: 40573.35948234932\n",
      "loss: 40566.04450709787\n",
      "loss: 40559.15612430092\n",
      "loss: 40552.660362104034\n",
      "loss: 40546.52665452149\n",
      "loss: 40540.72743088171\n",
      "loss: 40535.2377631436\n",
      "loss: 40530.035061775045\n",
      "loss: 40525.09881253558\n",
      "loss: 40520.41034788007\n",
      "loss: 40515.95264775808\n",
      "loss: 40511.710165500604\n",
      "loss: 40507.66867516439\n",
      "loss: 40503.81513731917\n",
      "loss: 40500.1375807166\n",
      "loss: 40496.6249977026\n",
      "loss: 40493.26725153579\n",
      "loss: 40490.05499408075\n",
      "loss: 40486.979592519405\n",
      "loss: 40484.033063982286\n",
      "loss: 40481.208017104174\n",
      "loss: 40478.49759966957\n",
      "loss: 40475.895451633776\n",
      "loss: 40473.395662866154\n",
      "loss: 40470.99273511757\n",
      "loss: 40468.681547675566\n",
      "loss: 40466.45732635539\n",
      "loss: 40464.31561541506\n",
      "loss: 40462.25225210792\n",
      "loss: 40460.26334358092\n",
      "loss: 40458.34524587664\n",
      "loss: 40456.49454481101\n",
      "loss: 40454.708038563454\n",
      "loss: 40452.98272176886\n",
      "loss: 40451.31577100081\n",
      "loss: 40449.70453147956\n",
      "loss: 40448.146504905926\n",
      "loss: 40446.6393383102\n",
      "loss: 40445.180813814186\n",
      "loss: 40443.76883922409\n",
      "loss: 40442.40143938581\n",
      "loss: 40441.07674822319\n",
      "loss: 40439.793001401566\n",
      "loss: 40438.548529565094\n",
      "loss: 40437.341752091554\n",
      "loss: 40436.17117132568\n",
      "loss: 40435.0353672407\n",
      "loss: 40433.93299250407\n",
      "loss: 40432.86276791087\n",
      "loss: 40431.82347813388\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent\n",
    "b_user, b_item = gradient_descent(R_copy, b_user, b_item, irow, jcol, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ac82ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "def rmse(R, b_user, b_item, irow, jcol):\n",
    "    return np.sqrt(2 * loss(b_user, b_item, R, irow, jcol) / len(irow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5b9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non regularized loss function model RMSE: 0.9681046554908976\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE on test set\n",
    "print('Non regularized loss function model RMSE:' , rmse(R, b_user, b_item, test_irow, test_jcol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0afb74",
   "metadata": {},
   "source": [
    "## Regularized Loss Function Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797ce2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized loss function\n",
    "def loss_reg(b_user, b_item, R, irow, jcol, lmbda):\n",
    "    loss = 0\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        loss += (R[i, j] - b_user[i] - b_item[j]) ** 2 * 0.5 + lmbda/2 * (b_user[i] ** 2 + b_item[j] ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d009acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized gradient of loss function\n",
    "def gradient_reg(b_user, b_item, R, irow, jcol, lmbda):\n",
    "    b_user_grad = np.zeros(b_user.shape)\n",
    "    b_item_grad = np.zeros(b_item.shape)\n",
    "    for i, j in zip(irow, jcol):\n",
    "        if np.isnan(R[i, j]):\n",
    "            continue\n",
    "        b_user_grad[i] += (R[i, j] - b_user[i] - b_item[j]) - lmbda * b_user[i]\n",
    "        b_item_grad[j] += (R[i, j] - b_user[i] - b_item[j]) - lmbda * b_item[j]\n",
    "\n",
    "    return b_user_grad, b_item_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58b90ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized gradient descent function\n",
    "def gradient_descent_reg(R, b_user, b_item, irow, jcol, lmbda, lr, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        b_user_grad, b_item_grad = gradient_reg(b_user, b_item, R, irow, jcol, lmbda)\n",
    "        prev_b_user = b_user.copy()\n",
    "        prev_b_item = b_item.copy()\n",
    "        b_user += lr * b_user_grad\n",
    "        b_item += lr * b_item_grad\n",
    "        if epoch % 10 == 0:\n",
    "            print('loss:', loss_reg(b_user, b_item, R, irow, jcol, lmbda))\n",
    "        # early stopping\n",
    "        if np.linalg.norm(b_user - prev_b_user) < 1e-2 and np.linalg.norm(b_item - prev_b_item) < 1e-2:\n",
    "            break\n",
    "\n",
    "    return b_user, b_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06c6114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize biases\n",
    "b_user = np.random.randn(943)\n",
    "b_item = np.random.randn(1682)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c954a2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 557293.2071953361\n",
      "loss: 119586.96863083639\n",
      "loss: 80159.24880925246\n",
      "loss: 65938.9770708831\n",
      "loss: 58576.40308846983\n",
      "loss: 54129.40784699711\n",
      "loss: 51194.895939773305\n",
      "loss: 49142.24260557388\n",
      "loss: 47645.090662013936\n",
      "loss: 46517.769799933114\n",
      "loss: 45647.21740184988\n",
      "loss: 44960.94205268114\n",
      "loss: 44410.528771477344\n",
      "loss: 43962.54689256373\n",
      "loss: 43593.26208604325\n",
      "loss: 43285.423208603665\n",
      "loss: 43026.23667877849\n",
      "loss: 42806.04847098954\n",
      "loss: 42617.46259703246\n",
      "loss: 42454.73709677593\n",
      "loss: 42313.361250597685\n",
      "loss: 42189.753994110106\n",
      "loss: 42081.04514479395\n",
      "loss: 41984.914305713464\n",
      "loss: 41899.47063881205\n",
      "loss: 44725.88352299339\n",
      "loss: 44651.79789296341\n",
      "loss: 44591.85018016707\n",
      "loss: 44537.92343478617\n",
      "loss: 44489.02461967675\n",
      "loss: 44444.495785692474\n",
      "loss: 44403.80383267787\n",
      "loss: 44366.49995398292\n",
      "loss: 44332.20103662535\n",
      "loss: 44300.57756138863\n",
      "loss: 44271.344571003596\n",
      "loss: 44244.25453993302\n",
      "loss: 44219.091598715095\n",
      "loss: 44195.666790302865\n",
      "loss: 44173.81413969138\n",
      "loss: 44153.38737657552\n",
      "loss: 44134.257188431955\n",
      "loss: 44116.30890795645\n",
      "loss: 44099.44055832986\n",
      "loss: 44083.561194827416\n",
      "loss: 44068.58949286402\n",
      "loss: 44054.4525418582\n",
      "loss: 44041.08481158781\n",
      "loss: 44028.427263616984\n",
      "loss: 44016.4265851516\n",
      "loss: 72550.55349718498\n",
      "loss: 71760.29800879782\n",
      "loss: 71681.28578631063\n",
      "loss: 71640.15763175845\n",
      "loss: 71610.34106613479\n",
      "loss: 71586.07057403814\n",
      "loss: 71565.16808494682\n",
      "loss: 71546.5786731421\n",
      "loss: 71529.71082222335\n",
      "loss: 71514.19880248481\n",
      "loss: 71499.79966963084\n",
      "loss: 71486.34266791743\n",
      "loss: 71473.70208725498\n",
      "loss: 71461.78172117147\n",
      "loss: 71450.50550075548\n",
      "loss: 71439.8116070525\n",
      "loss: 71429.64863323641\n",
      "loss: 71419.97300137335\n",
      "loss: 71410.74717274663\n",
      "loss: 71401.93837502025\n",
      "loss: 71393.51767502683\n",
      "loss: 71385.45928827344\n",
      "loss: 71377.74005424394\n",
      "loss: 71370.33903014028\n",
      "loss: 71363.23717091916\n",
      "loss: 297346.34134231706\n",
      "loss: 257254.05471685185\n",
      "loss: 254449.5912333109\n",
      "loss: 253444.10077574322\n",
      "loss: 252960.16647791656\n",
      "loss: 252695.11727880166\n",
      "loss: 252538.1806108408\n",
      "loss: 252439.96984522356\n",
      "loss: 252375.7402456235\n",
      "loss: 252332.09036966105\n",
      "loss: 252301.35193086797\n",
      "loss: 252278.95532717326\n",
      "loss: 252262.09058721922\n",
      "loss: 252248.98500451486\n",
      "loss: 252238.49540259008\n",
      "loss: 252229.86980173926\n",
      "loss: 252222.60420672156\n",
      "loss: 252216.3546027214\n",
      "loss: 252210.88185325052\n",
      "loss: 252206.01663421775\n",
      "loss: 252201.63679140763\n",
      "loss: 252197.65252133211\n",
      "loss: 252193.99654635735\n",
      "loss: 252190.6175181478\n",
      "loss: 252187.4755320123\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization to find best lambda on validation set\n",
    "best_rmse = float('inf')\n",
    "best_lmbda = 0\n",
    "for lmbda in [0.001, 0.01, 0.1, 1]:\n",
    "    b_user, b_item = gradient_descent_reg(R_copy, b_user, b_item, irow, jcol, lmbda, 0.0005, 250)\n",
    "    rmse_val = rmse(R, b_user, b_item, val_irow, val_jcol)\n",
    "    if rmse_val < best_rmse:\n",
    "        best_rmse = rmse_val\n",
    "        best_lmbda = lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "600c0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07ce012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 93158.41211999886\n",
      "loss: 50743.181315307665\n",
      "loss: 47762.760693486816\n",
      "loss: 46696.94272497925\n",
      "loss: 46127.17723207204\n",
      "loss: 45771.600545916444\n",
      "loss: 45530.3739782717\n",
      "loss: 45357.8020473515\n",
      "loss: 45229.6200017272\n",
      "loss: 45131.65446872691\n",
      "loss: 45055.06260966397\n",
      "loss: 44994.051758474714\n",
      "loss: 44944.680107012246\n",
      "loss: 44904.18114891234\n",
      "loss: 44870.56263349545\n",
      "loss: 44842.35827430468\n",
      "loss: 44818.46857313212\n",
      "loss: 44798.0556856148\n",
      "loss: 44780.4721520854\n",
      "loss: 44765.21146463626\n",
      "loss: 44751.87307474996\n",
      "loss: 44740.13716948011\n",
      "loss: 44729.74619214669\n",
      "loss: 44720.49110651175\n",
      "loss: 44712.20105344817\n",
      "loss: 44704.735470952204\n",
      "loss: 44697.9780276167\n",
      "loss: 44691.83190787644\n",
      "loss: 44686.2161163274\n",
      "loss: 44681.06255826405\n",
      "loss: 44676.313716963\n",
      "loss: 44671.920793706246\n",
      "loss: 44667.84220939757\n",
      "loss: 44664.04239078416\n",
      "loss: 44660.49078214464\n",
      "loss: 44657.161036703794\n",
      "loss: 44654.030352074085\n",
      "loss: 44651.078921779146\n",
      "loss: 44648.28948074222\n",
      "loss: 44645.64692720777\n",
      "loss: 44643.13800709817\n",
      "loss: 44640.75104957606\n",
      "loss: 44638.475744773845\n"
     ]
    }
   ],
   "source": [
    "# Regularized gradient descent with best lambda\n",
    "b_user, b_item = gradient_descent_reg(R_test_mask, b_user, b_item, irow, jcol, best_lmbda, 0.0005, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63742afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for regularized RMSE on test set with best model\n",
    "def rmse_reg(R, b_user, b_item, irow, jcol, lmbda):\n",
    "    return np.sqrt(2 * loss_reg(b_user, b_item, R, irow, jcol, lmbda) / len(irow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02139e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized RMSE: 1.0033994861270819\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE on test set\n",
    "print('Regularized RMSE:' , rmse_reg(R, b_user, b_item, test_irow, test_jcol, best_lmbda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
